import json
import os
import yaml
import random
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from .models import SessionMetadata, Candidate, AuditResult, Violation, Ruleset, Severity, Rule

class EvoCore:
    def __init__(self, base_dir: str = "sessions"):
        self.base_dir = Path(base_dir)

    def init_session(self, name: str):
        session_dir = self.base_dir / name
        if session_dir.exists():
            raise FileExistsError(f"Session '{name}' already exists.")
        session_dir.mkdir(parents=True)
        
        metadata = SessionMetadata(name=name)
        with open(session_dir / "metadata.json", "w", encoding="utf-8") as f:
            f.write(metadata.model_dump_json(indent=2))
        
        print(f"Session '{name}' initialized at {session_dir}")

        # Create/Update requirements.json with new template
        req_file = Path("requirements.json")
        if not req_file.exists():
            template = {
              "project_name": f"{name}_project",
              "goal": "Explain the system goal here",
              "features": ["Feature A", "Feature B"],
              "traffic": {
                "peak_rps": 5000,
                "avg_rps": 800,
                "read_write_ratio": "70:30",
                "payload_size_kb": 2
              },
              "slo": {
                "p95_latency_ms": 200,
                "error_rate": 0.01,
                "availability": 0.999
              },
              "constraints": {
                "data_residency": "EU",
                "compliance": ["GDPR"],
                "monthly_budget_usd": 20000,
                "team_size": 2,
                "time_to_mvp_weeks": 4
              },
              "preferences": {
                "prefer_managed_services": True,
                "prefer_kubernetes": False,
                "languages": ["python", "node"],
                "cloud": "gcp|aws|azure|any"
              }
            }
            with open(req_file, "w", encoding="utf-8") as f:
                json.dump(template, f, indent=2)
            print("Created new questionnaire-style 'requirements.json'.")
        else:
            print("Using existing 'requirements.json'.")

    def generate(self, session_path: str, n: int = 3, reset: bool = False):
        session_dir = Path(session_path)
        candidates_dir = session_dir / "candidates"
        audits_dir = session_dir / "audits"
        metadata_file = session_dir / "metadata.json"
        
        if not session_dir.exists():
            raise FileNotFoundError(f"Session path '{session_path}' not found.")
            
        # Refreshed metadata
        metadata = {
            "name": session_dir.name, 
            "created_at": str(datetime.now()),
            "generation_stats": {"attempts": 0, "failures": 0, "retries": 0}
        }
        
        # Reset Logic
        if reset:
            import shutil
            if candidates_dir.exists():
                shutil.rmtree(candidates_dir)
            if audits_dir.exists():
                shutil.rmtree(audits_dir)
            print("Resetting session: candidates and audits cleared.")
        
        candidates_dir.mkdir(exist_ok=True)
        
        req_file = Path("requirements.json")
        requirements = {}
        if req_file.exists():
            with open(req_file, "r", encoding="utf-8") as f:
                requirements = json.load(f)
        
        from .llm_gemini import generate_candidate
        from .models import Proposal, Architecture, Components, ComponentDetails, ReliabilityConfig, SLO, Acceptance, Experiments, ExperimentConfig, Risk, ChaosTest, Constraint
        
        def normalize_candidate(candidate: Candidate, requirements: Dict[str, Any]):
            if not candidate.proposal:
                return
            
            p = candidate.proposal
            notes = []
            
            # 1. Normalize Acceptance Constraints
            if not p.acceptance.hard_constraints:
                p.acceptance.hard_constraints = [
                    Constraint(metric="latency.p95_ms", op="<=", threshold=p.slo.p95_latency_ms),
                    Constraint(metric="errors.rate", op="<=", threshold=p.slo.error_rate)
                ]
                notes.append("Backfilled missing hard_constraints from SLO.")
                
            # 2. Normalize Load Test
            lt = p.experiments.load_test
            # Note: Pydantic fields might be None if Optional, but ExperimentConfig fields defined as int/str are required. 
            # If generated by Gemini via Pydantic model, they exist. 
            # However, if target_rps is optional in model (it is), we check it.
            if lt.target_rps is None:
                # default to 1000 or from requirements
                default_rps = 1000
                if "traffic" in requirements and "peak_rps" in requirements["traffic"]:
                    default_rps = int(requirements["traffic"]["peak_rps"])
                lt.target_rps = default_rps
                notes.append(f"Backfilled load_test.target_rps to {default_rps}")
                
            # duration_s is int in model, so it must exist to pass validation.
            # But if we want to enforce specific default if it was somehow 0 or low? 
            # User said: "If experiments.load_test missing duration_s" -> Model says duration_s is int (required).
            # So Gemini wouldn't have passed validation if it was missing. 
            # But let's assume we might relax model later or it passed with 0.
            # Let's check logic: if 0, update? Or just trust model validation?
            # User instruction: "If duration_s missing... default 900". 
            # Since strict Pydantic model requires it, it won't be "missing" in key sense, but maybe value sense?
            # Let's stick to target_rps which is Optional.
            
            # 3. Normalize Risks
            if not p.risks:
                p.risks.append(Risk(
                    title="Unknown risk: needs review",
                    mitigation="Run load/chaos tests and review dependencies"
                ))
                notes.append("Backfilled empty risks list.")
                
            # 4. Normalize Cache/Queue Types
            if p.architecture and p.architecture.components:
                comps = p.architecture.components
                if comps.cache is None:
                    # If None, create one with type="none"? Model says cache is Optional[ComponentDetails]
                    # User says: "If cache.type missing: default 'none'"
                    # If component itself is missing, let's leave it None or set to type=none?
                    # Rule `is_cache_missing` checks if it is None OR type=="none".
                    # So leaving it None is fine for logic, but for "Complete Structure" maybe user wants explicit object?
                    # Let's leave it as None if None. 
                    # WAIT, `is_cache_missing` returns True if None. 
                    # If we want to "fix" it? No, user normalized logic validation triggers.
                    # Normalized only "补齐". If cache is None, it means no cache component.
                    # If user means "if cache component exists but type is missing" -> Impossible with strict Pydantic (type is str).
                    # Maybe user implies "If architecture.components.cache is None => set it to type='none'"?
                    pass
                
                # Check Queue
                # Same logic.
            
            p.normalization_notes.extend(notes)

        candidates = []
        strategies = ["cost-optimized", "reliability-optimized", "throughput-optimized"]
        for i in range(1, n + 1):
            metadata["generation_stats"]["attempts"] += 1
            strategy = strategies[(i-1) % len(strategies)]
            print(f"Generating candidate {i}/{n} (Strategy: {strategy})...")
            
            # Call Gemini (singular)
            candidate = generate_candidate(requirements, strategy_hint=strategy)
            
            if candidate:
                normalize_candidate(candidate, requirements)
                candidates.append(candidate)
            else:
                metadata["generation_stats"]["failures"] += 1
                print("Gemini generation unavailable or failed. Using fallback template.")
                
                # Fallback Template Logic (Hard-Rule Compliant V2)
                candidate = Candidate(
                    id=f"fallback_{i}_{int(datetime.now().timestamp())}",
                    proposal=Proposal(
                        title=f"Fallback Compliant Architecture {i}",
                        summary="Generated via fallback template to meet HARD rules.",
                        architecture=Architecture(
                            style="monolith" if i == 1 else "microservices",
                            components=Components(
                                api=ComponentDetails(type="http", instances=2),
                                db=ComponentDetails(type="postgres"),
                                cache=ComponentDetails(type="redis"), # Required by Logic? No, but good practice
                                queue=ComponentDetails(type="rabbitmq") # Risk check needs risk mitigation
                            ),
                            reliability=ReliabilityConfig(
                                timeouts_ms={"client": 2000, "server": 1000}, # Client > Server (R002 avoidance)
                                retries={"max_attempts": 2, "backoff": "exponential"}, # < 3 (R001 avoidance)
                                idempotency="required"
                            )
                        ),
                        slo=SLO(p95_latency_ms=200, error_rate=0.001, availability=99.9),
                        acceptance=Acceptance(
                            hard_constraints=[
                                {"metric": "latency.p95_ms", "op": "<=", "threshold": 200},
                                {"metric": "errors.rate", "op": "<=", "threshold": 0.01}
                            ]
                        ),
                        experiments=Experiments(
                            load_test=ExperimentConfig(tool="k6", duration_s=60, target_rps=100),
                            chaos_test=[ChaosTest(fault="pod_kill", target="api", duration_s=30)] # A001 compliance
                        ),
                        risks=[Risk(title="Queue Management", mitigation="DLQ and monitoring configured")] # R003 compliance
                    )
                )
                normalize_candidate(candidate, requirements)
                candidates.append(candidate)
        
        # Save candidates
        for cand in candidates:
            cand_file = candidates_dir / f"cand_{cand.id}.json"
        for cand in candidates:
            cand_file = candidates_dir / f"cand_{cand.id}.json"
            with open(cand_file, "w", encoding="utf-8") as f:
                f.write(cand.model_dump_json(indent=2))
            print(f"Saved candidate: {cand.id}")
            
        # Save Metadata
        if metadata_file.exists():
             with open(metadata_file, "r", encoding="utf-8") as f:
                 old_meta = json.load(f)
                 # merge or update? let's update for now
                 if isinstance(old_meta, dict):
                    old_meta.update(metadata)
                    metadata = old_meta
        
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2)

    def patch(self, session_path: str, candidate_id: str, apply_advice: bool = False):
        session_dir = Path(session_path)
        candidates_dir = session_dir / "candidates"
        cand_file = candidates_dir / f"cand_{candidate_id}.json"
        
        if not cand_file.exists():
             raise FileNotFoundError(f"Candidate file not found: {cand_file}")
             
        with open(cand_file, "r", encoding="utf-8") as f:
            cand = Candidate(**json.load(f))
            
        p = cand.proposal
        if not p:
            print("Candidate has no proposal data to patch.")
            return

        notes = []
        
        # R001 Fix: High Retries
        # Logic: if max_attempts >= 3 -> set to 1, backoff=exponential
        if p.architecture and p.architecture.reliability:
            rel = p.architecture.reliability
            retries = rel.retries
            current_attempts = 0
            if isinstance(retries, dict):
                current_attempts = int(retries.get("max_attempts", 0))
            
            if current_attempts >= 3:
                # Apply fix
                rel.retries = {"max_attempts": 1, "backoff": "exponential"}
                notes.append("Fixed R001: Set retries.max_attempts to 1 and backoff to exponential.")
        
        # R002 Fix: Client Timeout <= Server Timeout
        # Logic: client = max(server + 300, server * 2)
        if p.architecture and p.architecture.reliability:
            rel = p.architecture.reliability
            timeouts = rel.timeouts_ms
            client_to = timeouts.get("client", 0)
            server_to = timeouts.get("server", 0)
            
            if client_to <= server_to and server_to > 0:
                new_client = max(server_to + 300, server_to * 2)
                timeouts["client"] = new_client
                notes.append(f"Fixed R002: Increased client timeout to {new_client}ms (> server timeout {server_to}ms).")
        
        
        # R005 Fix: Retries without Idempotency
        # Logic: if retries > 0 and idempotency == 'not_supported' -> set to 'recommended', add Risk
        if p.architecture and p.architecture.reliability:
            rel = p.architecture.reliability
            retries = rel.retries
            attempts = 0
            if isinstance(retries, dict):
                 attempts = int(retries.get("max_attempts", 0))
            
            idempotency = rel.idempotency
            if attempts > 0 and idempotency == "not_supported":
                rel.idempotency = "recommended"
                from .models import Risk
                p.risks.append(Risk(
                    title="Idempotency Risk",
                    mitigation="Retries detected. Idempotency-Key recommended to prevent double-writes."
                ))
                notes.append("Fixed R005: Set idempotency to 'recommended' and added Risk due to active retries.")

        # A002 Fix: Cache Missing
        # Logic: If cache missing, add Risk. If apply_advice is True, inject Redis.
        if p.architecture and p.architecture.components:
             # Check if cache missing (Logic from audit: None or type=='none')
             cache = p.architecture.components.cache
             is_missing = False
             if cache is None: is_missing = True
             elif isinstance(cache, dict) and cache.get("type", "none") == "none": is_missing = True
             elif hasattr(cache, "type") and cache.type == "none": is_missing = True
             
             if is_missing:
                 # Default behavior: Add Risk
                 from .models import Risk
                 p.risks.append(Risk(
                     title="Missing Cache Layer", 
                     mitigation="Cache unenabled: may impact read performance. Evaluate Redis for high-read paths."
                 ))
                 notes.append("Fixed A002: Added Risk entry for missing cache.")
                 
                 # Optional behavior: Apply Advice
                 if apply_advice:
                     # Inject Redis
                     if cache is None:
                         # Need to import ComponentDetails or construct dict
                         p.architecture.components.cache = {"type": "redis", "notes": "Injected by evo patch --apply-advice"}
                     elif isinstance(cache, dict):
                         cache["type"] = "redis"
                         cache["notes"] = "Injected by evo patch --apply-advice"
                     elif hasattr(cache, "type"):
                         cache.type = "redis"
                         cache.notes = "Injected by evo patch --apply-advice"
                         
                     notes.append("Applied Advice A002: Injected Redis cache component.")
        
        if not notes:
            print("No patchable violations found or candidate already compliant.")
            return
            
        # Update metadata
        cand.proposal.patch_notes.extend(notes)
        original_id = cand.id
        cand.id = f"{original_id}_patched"
        cand.created_at = datetime.now() # update timestamp? maybe keep original? let's update
        
        # Save
        patched_file = candidates_dir / f"cand_{cand.id}.json"
        with open(patched_file, "w", encoding="utf-8") as f:
            f.write(cand.model_dump_json(indent=2))
        
        print(f"Patched candidate saved to {patched_file}")
        print("Changes applied:")
        for n in notes:
            print(f"- {n}")

    def diff(self, session_path: str, base_id: str, target_id: str):
        session_dir = Path(session_path)
        candidates_dir = session_dir / "candidates"
        compare_dir = session_dir / "compare"
        compare_dir.mkdir(exist_ok=True)
        
        base_file = candidates_dir / f"cand_{base_id}.json"
        target_file = candidates_dir / f"cand_{target_id}.json"
        
        if not base_file.exists(): raise FileNotFoundError(f"Base candidate {base_id} not found")
        if not target_file.exists(): raise FileNotFoundError(f"Target candidate {target_id} not found")
        
        with open(base_file, "r", encoding="utf-8") as f: base = Candidate(**json.load(f))
        with open(target_file, "r", encoding="utf-8") as f: target = Candidate(**json.load(f))
        
        changes = []
        
        # Helper to compare simple paths
        def check_change(path, b_val, t_val, reason_guess=""):
            if b_val != t_val:
                changes.append({
                    "path": path,
                    "from": b_val,
                    "to": t_val,
                    "reason": reason_guess
                })

        p_base = base.proposal
        p_target = target.proposal
        
        # reliability.retries.max_attempts
        b_retries = 0
        if isinstance(p_base.architecture.reliability.retries, dict):
            b_retries = int(p_base.architecture.reliability.retries.get("max_attempts", 0))
        
        t_retries = 0
        if isinstance(p_target.architecture.reliability.retries, dict):
             t_retries = int(p_target.architecture.reliability.retries.get("max_attempts", 0))
        
        if b_retries != t_retries:
            reason = "R001" if t_retries < b_retries and b_retries >= 3 else "Manual Adjustment"
            check_change("architecture.reliability.retries.max_attempts", b_retries, t_retries, reason)
            
        # reliability.timeouts_ms.client
        b_client_to = p_base.architecture.reliability.timeouts_ms.get("client", 0)
        t_client_to = p_target.architecture.reliability.timeouts_ms.get("client", 0)
        
        if b_client_to != t_client_to:
            reason = "R002" if t_client_to > b_client_to else "Manual Adjustment"
            check_change("architecture.reliability.timeouts_ms.client", b_client_to, t_client_to, reason)

        # reliability.idempotency
        b_idem = p_base.architecture.reliability.idempotency
        t_idem = p_target.architecture.reliability.idempotency
        if b_idem != t_idem:
            reason = "R005" if t_idem in ["recommended", "required"] and b_idem == "not_supported" else "Manual Adjustment"
            check_change("architecture.reliability.idempotency", b_idem, t_idem, reason)
            
        # components.cache.type
        def get_cache_type(prop):
            if not prop.architecture or not prop.architecture.components: return "none"
            c = prop.architecture.components.cache
            if c is None: return "none"
            if isinstance(c, dict): return c.get("type", "none")
            return c.type
            
        b_cache = get_cache_type(p_base)
        t_cache = get_cache_type(p_target)
        
        if b_cache != t_cache:
            reason = "A002" if t_cache != "none" and b_cache == "none" else "Manual Adjustment"
            check_change("architecture.components.cache.type", b_cache, t_cache, reason)

        # Generate outputs
        diff_id = f"{base_id}_vs_{target_id}"
        
        # JSON
        json_out = {
            "base": base_id,
            "target": target_id,
            "strategies": {"base": p_base.strategy, "target": p_target.strategy},
            "changes": changes,
            "patch_notes": p_target.patch_notes
        }
        json_path = compare_dir / f"diff_{diff_id}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_out, f, indent=2)
            
        # Markdown
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        md_lines = [
            f"# Diff: {base_id} vs {target_id}",
            f"Date: {timestamp}",
            f"**Strategies**: {p_base.strategy} -> {p_target.strategy}",
            "",
            "## Change Summary"
        ]
        
        if not changes:
            md_lines.append("No structural changes detected.")
        else:
            for c in changes:
                md_lines.append(f"- **{c['path']}**: `{c['from']}` -> `{c['to']}` ({c['reason']})")
        
        if p_target.patch_notes:
            md_lines.append("")
            md_lines.append("## Patch Notes (Target)")
            for note in p_target.patch_notes:
                md_lines.append(f"- {note}")
                
        md_path = compare_dir / f"diff_{diff_id}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))
            
        print(f"Diff generated at {md_path}")


    def audit(self, session_path: str, include_patched: bool = False):
        session_dir = Path(session_path)
        candidates_dir = session_dir / "candidates"
        legacy_file = session_dir / "candidates.json"
        
        # Migration logic
        if legacy_file.exists() and not candidates_dir.exists():
            print("Detecting legacy format. Migrating to 'candidates/' directory...")
            candidates_dir.mkdir()
            with open(legacy_file, "r", encoding="utf-8") as f:
                legacy_data = json.load(f)
                for item in legacy_data:
                    c = Candidate(**item)
                    cand_path = candidates_dir / f"cand_{c.id}.json"
                    with open(cand_path, "w", encoding="utf-8") as cf:
                        cf.write(c.model_dump_json(indent=2))
            print("Migration complete.")
        
        if not candidates_dir.exists():
            raise FileNotFoundError("candidates/ directory not found in session.")
            
        ruleset_file = Path("ruleset.yaml")
        if not ruleset_file.exists():
            raise FileNotFoundError("ruleset.yaml not found in current directory.")

        with open(ruleset_file, "r", encoding="utf-8") as f:
            rules_data = yaml.safe_load(f)
            # Ensure Rule loading handles optional fields
            rules = []
            for r in rules_data['rules']:
                rules.append(Rule(**r))
            ruleset = Ruleset(rules=rules)

        # Load candidates from files
        candidates = []
        for cand_file in candidates_dir.glob("cand_*.json"):
            # Filter patched candidates unless requested
            if not include_patched and "_patched" in cand_file.name:
                continue
                
            with open(cand_file, "r", encoding="utf-8") as f:
                try:
                    candidates.append(Candidate(**json.load(f)))
                except Exception as e:
                    print(f"Error loading candidate {cand_file}: {e}")

        # Clean audits directory to ensure report matches current candidates
        import shutil
        audits_dir = session_dir / "audits"
        if audits_dir.exists():
            shutil.rmtree(audits_dir)
        audits_dir.mkdir(exist_ok=True)

        results = []
        for cand in candidates:
            violations = []
            
            # Helper context for eval
            # We construct a secure evaluation context
            # Functions allow the ruleset to remain declarative but powerful
            
            proposal = cand.proposal
            
            def acceptance_has_metrics(required_metrics):
                if not proposal or not proposal.acceptance: return False
                existing = [c.metric for c in proposal.acceptance.hard_constraints]
                return all(m in existing for m in required_metrics)

            def get_retry_max_attempts():
                if not proposal: return 0
                
                # Check architecture.reliability exists
                if not proposal.architecture or not proposal.architecture.reliability:
                    return 0
                
                retries = proposal.architecture.reliability.retries
                 # retries can be dict or object depending on load, pydantic model enforces Dict though
                # but let's be safe
                if isinstance(retries, dict):
                    return int(retries.get("max_attempts", 0))
                return 0

            def get_timeout(typ):
                if not proposal or not proposal.architecture or not proposal.architecture.reliability: return 0
                return proposal.architecture.reliability.timeouts_ms.get(typ, 0)
            
            def queue_used_requires_risks():
                if not proposal or not proposal.architecture or not proposal.architecture.components: return False
                # Check if queue is used
                q = proposal.architecture.components.queue
                queue_active = False
                if q:
                     # Check type via attribute or dict logic if accessed dynamically, but Pydantic object access is via dot
                     # Wait, ComponentDetails is object. 
                     if isinstance(q, dict):
                         queue_active = q.get("type", "none") != "none"
                     else:
                         queue_active = q.type != "none"
                
                if queue_active:
                    return len(proposal.risks) == 0 # Violation if risks empty
                return False

            def has_meaningful_chaos_test():
                if not proposal or not proposal.experiments: return False
                return len(proposal.experiments.chaos_test) > 0

            def is_cache_missing():
                if not proposal or not proposal.architecture or not proposal.architecture.components: return True
                c = proposal.architecture.components.cache
                if not c: return True
                if isinstance(c, dict): return c.get("type", "none") == "none"
                return c.type == "none"

            eval_context = {
                "proposal": proposal,
                "acceptance_has_metrics": acceptance_has_metrics,
                "get_retry_max_attempts": get_retry_max_attempts,
                "get_timeout": get_timeout,
                "queue_used_requires_risks": queue_used_requires_risks,
                "has_meaningful_chaos_test": has_meaningful_chaos_test,
                "is_cache_missing": is_cache_missing
            }

            if proposal:
                for rule in ruleset.rules:
                    try:
                        # Safe eval might be needed in production, for MVP eval is acceptable for local tools
                        condition_met = eval(rule.condition, {"__builtins__": {}}, eval_context)
                        
                        # Rule semantics: condition True means "Compliance" or "Violation"?
                        # Usually condition describes the "Requirement". So True = Good.
                        # BUT, looking at ruleset:
                        # H001: "proposal... is not None" -> True is Good.
                        # R001: "retries >= 3" -> True is BAD (Risk).
                        
                        # Let's check IDs.
                        # H-series: Essential Requirements (True = Pass)
                        # R-series: Risks (True = Fail/Warning)
                        # A-series: Advice (True = Trigger Suggestion? Or True = Good state?)
                        
                        # Let's standardize:
                        # H*: Condition is success criteria. If False -> Violation.
                        # R*: Condition is risk presence. If True -> Violation.
                        # A*: Condition is negative state? 
                        #     A001: "has_meaningful...() == False" -> True means missing.
                        #     A002: "is_cache_missing()" -> True means missing.
                         
                        is_violation = False
                        
                        if rule.id.startswith("H"):
                            if not condition_met: is_violation = True
                        elif rule.id.startswith("R"):
                            if condition_met: is_violation = True
                        elif rule.id.startswith("A"):
                            if condition_met: is_violation = True
                        
                        if is_violation:
                            violations.append(Violation(
                                rule_id=rule.id,
                                severity=rule.severity,
                                message=rule.description,
                                fix_suggestion=rule.fix_suggestion
                            ))
                            
                    except Exception as e:
                        print(f"Error evaluating rule {rule.id}: {e}")
            else:
                # Fallback auditing for v1 or error
                pass 

            passed = not any(v.severity == Severity.HARD for v in violations)
            score = 100 - (len(violations) * 10) 
            
            result = AuditResult(
                candidate_id=cand.id,
                passed=passed,
                violations=violations,
                score=max(0, score)
            )
            results.append(result)
            
            # Save individual audit result
            audit_path = audits_dir / f"audit_{result.candidate_id}.json"
            with open(audit_path, "w", encoding="utf-8") as f:
                f.write(result.model_dump_json(indent=2))
        
        # Save summary for compatibility
        audit_file = session_dir / "audit_results.json"
        with open(audit_file, "w", encoding="utf-8") as f:
            json.dump([r.model_dump(mode='json') for r in results], f, indent=2)
        
        print(f"Audit completed. Results saved to {audits_dir}")

    def report(self, session_path: str, include_patched: bool = False):
        session_dir = Path(session_path)
        audits_dir = session_dir / "audits"
        
        results = []
        if audits_dir.exists():
            for audit_file in audits_dir.glob("audit_*.json"):
                with open(audit_file, "r", encoding="utf-8") as f:
                    try:
                        res = AuditResult(**json.load(f))
                        # Filter patched candidates logic (audit results might exist even if not valid for this report view)
                        # We filter based on candidate_id in the result
                        if not include_patched and "_patched" in res.candidate_id:
                            continue
                        results.append(res)
                    except Exception as e:
                        print(f"Error loading audit result {audit_file}: {e}")
        else:
             # Fallback to summary file if audits dir doesn't exist (legacy)
             audit_file = session_dir / "audit_results.json"
             if not audit_file.exists():
                 raise FileNotFoundError("No audit results found.")
             with open(audit_file, "r", encoding="utf-8") as f:
                 data = json.load(f)
                 results = [AuditResult(**r) for r in data]
            
        report_lines = ["# Session Audit Report", f"Date: {datetime.now()}", "", "## Summary"]
        
        passed_count = sum(1 for r in results if r.passed)
        total = len(results)
        report_lines.append(f"Total Candidates: {total}")
        report_lines.append(f"Passed: {passed_count}")
        report_lines.append(f"Failed: {total - passed_count}")
        report_lines.append("")
        
        report_lines.append("## Detailed Results")
        for r in results:
            status = "PASS" if r.passed else "FAIL"
            icon = "✅" if r.passed else "❌"
            patched_tag = " (PATCHED)" if "_patched" in r.candidate_id else ""
            report_lines.append(f"### Candidate {r.candidate_id} {icon} ({status}){patched_tag}")
            report_lines.append(f"- **Score**: {r.score}")
            
            # Attempt to read strategy from candidate file
            cand_path = session_dir / "candidates" / f"cand_{r.candidate_id}.json"
            if cand_path.exists():
                try:
                    with open(cand_path, "r", encoding="utf-8") as f:
                        c_data = json.load(f)
                        strat = c_data.get("proposal", {}).get("strategy", "Unknown")
                        report_lines.append(f"- **Strategy**: {strat}")
                except:
                    pass
            
            if r.violations:
                report_lines.append("- **Violations**:")
                for v in r.violations:
                    report_lines.append(f"  - [{v.severity.value}] {v.rule_id}: {v.message}")
                    if v.fix_suggestion:
                        report_lines.append(f"    Fix: {v.fix_suggestion}")
            else:
                report_lines.append("- No violations found.")
                
            # Check for Diffs
            compare_dir = session_dir / "compare"
            if compare_dir.exists():
                diff_files = list(compare_dir.glob(f"diff_*_vs_{r.candidate_id}.md"))
                if diff_files:
                    report_lines.append("- **Comparison**:")
                    for df in diff_files:
                        # Extract base ID
                        # Filename: diff_<base>_vs_<target>.md
                        # base = filename.replace("diff_", "").replace(f"_vs_{r.candidate_id}.md", "")
                        # Let's just link it
                        report_lines.append(f"  - [View Diff]({df.name})")
            
            report_lines.append("")

        report_file = session_dir / "report.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
            
        print(f"Report generated at {report_file}")
